
@inproceedings{Jin2020LocalOptimality,
  title        = {What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?},
  author       = {Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I.},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  series       = {PMLR},
  year         = {2020},
}

@article{Han2023RiemannianMinimax,
  title        = {Nonconvex-Nonconcave Min-Max Optimization on Riemannian Manifolds},
  author       = {Han, Andi and others},
  journal      = {Transactions on Machine Learning Research},
  year         = {2023},
}

@inproceedings{KimSeo2022SemiImplicit,
  title        = {Semi-Implicit Hybrid Gradient Methods with Application to Adversarial Robustness},
  author       = {Kim, Beomsu and Seo, Junghoon},
  booktitle    = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  series       = {PMLR},
  year         = {2022},
}

@inproceedings{Bukharin2023RobustMARL,
  title        = {Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms},
  author       = {Bukharin, Alexander and others},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume       = {36},
  pages        = {68121--68133},
  year         = {2023},
}


@inproceedings{goldberg2022ppad,
  title={PPAD-complete pure approximate Nash equilibria in Lipschitz games},
  author={Goldberg, Paul W and Katzman, Matthew},
  booktitle={International Symposium on Algorithmic Game Theory},
  pages={169--186},
  year={2022},
  organization={Springer}
}

@article{papadimitriou2022computational,
  title={The computational complexity of multi-player concave games and kakutani fixed points},
  author={Papadimitriou, Christos H and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Zampetakis, Manolis},
  journal={arXiv preprint arXiv:2207.07557},
  year={2022}
}

@article{ieor2011linear,
  title={The Linear Complementarity Problem, Lemke Algorithm, Perturbation, and the Complexity Class PPAD},
  author={IEOR, UC},
  year={2011}
}

@inproceedings{daskalakis2021complexity,
  title={The complexity of constrained min-max optimization},
  author={Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1466--1478},
  year={2021}
}

@Article{Recht-Re_conj_is_false_ICML_2020,
  author  = {Lai, Zehua and Lim, Lek-Heng},
  journal = {arXiv preprint arXiv:2006.01510},
  title   = {{Recht-R\'{e} Noncommutative Arithmetic-Geometric Mean Conjecture is False}},
  year    = {2020},
}

@InProceedings{RR-conjecture2012,
  author    = {Recht, Benjamin and R\'{e}, Christopher},
  booktitle = {Proceedings of the 25th Annual Conference on Learning Theory},
  title     = {{Toward a noncommutative arithmetic-geometric mean inequality: Conjectures, case-studies, and consequences.}},
  year      = {2012},
  editor    = {Mannor, S. and Srebro, N. and Williamson, R. C.},
  note      = {Edinburgh, Scotland},
  pages     = {11.1–11.24},
  volume    = {23},
  edition   = {Proceedings of Machine Learning Research},
}

@Article{Li2019,
  author  = {Xiao Li and Zhihui Zhu and Anthony Man-Cho So and Jason D Lee},
  title   = {{Incremental Methods for Weakly Convex Optimization}},
  year    = {2019},
  journal = {arXiv preprint arXiv:1907.11687},
}

@article{GhadimiLan13,
   title={{Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming}},
   volume={23},
   ISSN={1095-7189},
   DOI={10.1137/120880811},
   number={4},
   journal={SIAM Journal on Optimization},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   author={Ghadimi, Saeed and Lan, Guanghui},
   year={2013},
   month={Jan},
   pages={2341–2368}
}

@inproceedings{Ying2019,
    author = {Ying, Bicheng and Yuan, Kun and Vlaski, Stefan and Sayed, Ali H.},
    booktitle = {IEEE Transactions on Signal Processing},
    doi = {10.1109/TSP.2018.2878551},
    eprint = {1803.07964v2},
    issn = {1053587X},
    number = {2},
    pages = {474--489},
    title = {{Stochastic Learning Under Random Reshuffling With Constant Step-Sizes}},
    volume = {67},
    year = {2019}
}

@InProceedings{haochen2018random,
  title = 	 {{Random Shuffling Beats {SGD} after Finite Epochs}},
  author = 	 {Haochen, Jeff and Sra, Suvrit},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2624--2633},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR}
}

@article{Gurbuzbalaban2019IG,
  author = {G\"{u}rb\"{u}zbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo A.},
  title = {{Convergence Rate of Incremental Gradient and Incremental Newton Methods}},
  journal = {SIAM Journal on Optimization},
  volume = {29},
  number = {4},
  pages = {2542-2565},
  year = {2019},
  doi = {10.1137/17M1147846}
}

@article{Khaled2020,
    title={{Better Theory for SGD in the Nonconvex World}},
    author={Ahmed Khaled and Peter Richt\'{a}rik},
    year={2020},
    journal={arXiv preprint arXiv:2002.03329}
}

@article{Bottou2018,
  author =        {Bottou, L{\'e}on and Curtis, Frank E. and
                   Nocedal, Jorge},
  journal =       {SIAM Review},
  number =        {2},
  pages =         {223-311},
  title =         {{Optimization Methods for Large-Scale Machine
                   Learning}},
  volume =        {60},
  year =          {2018},
  doi =           {10.1137/16M1080173},
}

@article{Nguyen2020,
    title={{A Unified Convergence Analysis for Shuffling-Type Gradient Methods}},
    author={Lam M. Nguyen and Quoc Tran-Dinh and Dzung T. Phan and Phuong Ha Nguyen and Marten van Dijk},
    year={2020},
    journal={arXiv preprint arXiv:2002.08246}
}

@InProceedings{Nagaraj2019,
  title = 	 {{SGD without Replacement: Sharper Rates for General Smooth Convex Functions}},
  author = 	 {Nagaraj, Dheeraj and Jain, Prateek and Netrapalli, Praneeth},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4703--4711},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR}
}

@article{Rajput2020,
    title={{Closing the convergence gap of SGD without replacement}},
    author={Shashank Rajput and Anant Gupta and Dimitris Papailiopoulos},
    year={2020},
    journal={arXiv preprint arXiv:2002.10400}
}

@Article{Khaled2019a,
  author  = {Ahmed Khaled and Konstantin Mishchenko and Peter Richt{\'a}rik},
  title   = {{First Analysis of Local GD on Heterogeneous Data}},
  journal = {arXiv preprint arXiv:1909.04715},
  year    = {2019},
}

@Article{Karimireddy2019,
  author  = {Sai Praneeth Karimireddy and Satyen Kale and Mehryar Mohri and Sashank J. Reddi and Sebastian U. Stich and Ananda Theertha Suresh},
  title   = {{SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning}},
  journal = {arXiv preprint arXiv:1910.06378},
  year    = {2019},
}

@InProceedings{Gower2019,
  author    = {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt\'{a}rik, Peter},
  title     = {{SGD}: {General Analysis and Improved Rates}},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {5200--5209},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
}

@InCollection{Bertsekas2011,
  author    = {Dimitri P. Bertsekas},
  title     = {{Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey}},
  booktitle = {Optimization for Machine Learning},
  publisher = {The MIT Press},
  year      = {2011},
  editor    = {Suvrit Sra and Sebastan Nowozin and Stephen J. Wright},
  chapter   = {4},
  isbn      = {9780262298773},
}

@Unpublished{Bottou2009,
  author = {Bottou, L\'{e}on},
  title  = {Curiously fast convergence of some stochastic gradient descent algorithms},
  note   = {Unpublished open problem offered to the attendance of the SLDS 2009 conference},
  year   = {2009},
  url    = {http://leon.bottou.org/papers/bottou-slds-open-problem-2009},
}

@Article{Gurbuzbalaban2019RR,
  author    = {G\"{u}rb\"{u}zbalaban, Mert and \"{O}zda\u{g}lar, Asuman and Parrilo, Pablo A.},
  title     = {{Why random reshuffling beats stochastic gradient descent}},
  journal   = {Mathematical Programming},
  year      = {2019},
  month     = {Oct},
  issn      = {1436-4646},
  doi       = {10.1007/s10107-019-01440-w},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{Shamir2016,
  author    = {Shamir, Ohad},
  title     = {{Without-Replacement Sampling for Stochastic Gradient Methods}},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  year      = {2016},
  series    = {NIPS’16},
  pages     = {46–54},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  isbn      = {9781510838819},
  location  = {Barcelona, Spain},
  numpages  = {9},
}

@Article{Meng2019,
  author        = {Qi Meng and Wei Chen and Yue Wang and Zhi-Ming Ma and Tie-Yan Liu},
  title         = {{Convergence analysis of distributed stochastic gradient descent with shuffling}},
  journal       = {Neurocomputing},
  year          = {2019},
  volume        = {337},
  pages         = {46 - 57},
  issn          = {0925-2312},
  __markedentry = {[rka:6]},
  doi           = {https://doi.org/10.1016/j.neucom.2019.01.037},
}

@InProceedings{Raman2019,
  author    = {Raman, Parameswaran and Srinivasan, Sriram and Matsushima, Shin and Zhang, Xinhua and Yun, Hyokun and Vishwanathan, S.V.N.},
  title     = {{Scaling Multinomial Logistic Regression via Hybrid Parallelism}},
  booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year      = {2019},
  series    = {KDD ’19},
  pages     = {1460–1470},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  doi       = {10.1145/3292500.3330837},
  isbn      = {9781450362016},
  location  = {Anchorage, AK, USA},
  numpages  = {11},
}

@InProceedings{Recht2012,
  author    = {Benjamin Recht and Christopher R{\'{e}}},
  title     = {{Toward a Noncommutative Arithmetic-geometric Mean Inequality: Conjectures, Case-studies, and Consequences}},
  booktitle = {Proceedings of the 25th Annual Conference on Learning Theory},
  year      = {2012},
  editor    = {Shie Mannor and Nathan Srebro and Robert C. Williamson},
  volume    = {23},
  series    = {Proceedings of Machine Learning Research},
  pages     = {11.1--11.24},
  address   = {Edinburgh, Scotland},
  month     = {25--27 Jun},
  publisher = {PMLR},
}

@InProceedings{Ying2018,
  author    = {B. {Ying} and K. {Yuan} and A. H. {Sayed}},
  title     = {Convergence of Variance-Reduced Learning Under Random Reshuffling},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2018},
  pages     = {2286-2290},
}

@Article{Mokhtari2018,
  author  = {Mokhtari, Aryan and G\"{u}rb\"{u}zbalaban, Mert and Ribeiro, Alejandro},
  title   = {{Surpassing Gradient Descent Provably: A Cyclic Incremental Method with Linear Convergence Rate}},
  journal = {SIAM Journal on Optimization},
  year    = {2018},
  volume  = {28},
  number  = {2},
  pages   = {1420-1447},
  doi     = {10.1137/16M1101702},
}

@Article{Gurbuzbalaban2017,
  author    = {G\"{u}rb\"{u}zbalaban, M. and Ozdaglar, A. and Parrilo, P. A.},
  title     = {{On the Convergence Rate of Incremental Aggregated Gradient Algorithms}},
  journal   = {SIAM Journal on Optimization},
  year      = {2017},
  volume    = {27},
  number    = {2},
  pages     = {1035–1048},
  month     = {Jan},
  issn      = {1095-7189},
  doi       = {10.1137/15m1049695},
  publisher = {Society for Industrial \& Applied Mathematics (SIAM)},
}

@Article{RuoyuSun2019,
  author    = {Ruoyu Sun and Yinyu Ye},
  title     = {{Worst-case Complexity of Cyclic Coordinate Descent: O($n^2$) Gap with Randomized Version}},
  journal   = {Mathematical Programming},
  year      = {2019},
  month     = oct,
  doi       = {10.1007/s10107-019-01437-5},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Lee2018,
  author    = {{Ching-pei} Lee and Stephen J. Wright},
  title     = {Random permutations fix a worst case for cyclic coordinate descent},
  journal   = {{IMA} Journal of Numerical Analysis},
  year      = {2018},
  volume    = {39},
  number    = {3},
  pages     = {1246--1275},
  month     = jul,
  doi       = {10.1093/imanum/dry040},
  publisher = {Oxford University Press ({OUP})},
}

@article{Recht2013,
  doi = {10.1007/s12532-013-0053-8},
  year = {2013},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {5},
  number = {2},
  pages = {201--226},
  author = {Benjamin Recht and Christopher R{\'{e}}},
  title = {{Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion}},
  journal = {Mathematical Programming Computation}
}

@Article{Sun2020,
  author={Sun, Ruo-Yu},
  title={{Optimization for Deep Learning: An Overview}},
  journal={Journal of the Operations Research Society of China},
  year={2020},
  month={Jun},
  day={01},
  volume={8},
  number={2},
  pages={249-294},
  issn={2194-6698},
  doi={10.1007/s40305-020-00309-6},
}

@Article{Stich2019b,
  author      = {Sebastian U. Stich},
  title       = {{Unified Optimal Analysis of the (Stochastic) Gradient Method}},
  journal     = {arXiv preprint arXiv:1907.04232},
  year        = {2019},
  date        = {2019-07-09},
  eprint      = {1907.04232v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1907.04232v2:PDF},
  keywords    = {cs.LG, cs.NA, math.NA, math.OC, stat.ML},
}

@article{Luo1991,
  doi = {10.1162/neco.1991.3.2.226},
  year = {1991},
  month = jun,
  publisher = {{MIT} Press - Journals},
  volume = {3},
  number = {2},
  pages = {226--245},
  author = {Zhi-Quan Luo},
  title = {{On the Convergence of the LMS Algorithm with Adaptive Learning Rate for Linear Feedforward Networks}},
  journal = {Neural Computation}
}

@article{Grippo1994,
  doi = {10.1080/10556789408805583},
  year = {1994},
  month = jan,
  publisher = {Informa {UK} Limited},
  volume = {4},
  number = {2},
  pages = {135--150},
  author = {Luigi Grippo},
  title = {{A class of unconstrained minimization methods for neural network training}},
  journal = {Optimization Methods and Software}
}

@article{Mangasarian1994,
  author = {Olvi L.   Mangasarian  and    Mikhail V.   Solodov},
  title = {{Serial and parallel backpropagation convergence via nonmonotone perturbed minimization}},
  journal = {Optimization Methods and Software},
  volume = {4},
  number = {2},
  pages = {103-116},
  year  = {1994},
  publisher = {Taylor & Francis},
  doi = {10.1080/10556789408805581}
}

@article{Bertsekas2000,
  doi = {10.1137/s1052623497331063},
  year = {2000},
  month = jan,
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  volume = {10},
  number = {3},
  pages = {627--642},
  author = {Dimitri P. Bertsekas and John N. Tsitsiklis},
  title = {{Gradient Convergence in Gradient methods with Errors}},
  journal = {{SIAM} Journal on Optimization}
}

@article{Nedic2001,
  doi = {10.1137/s1052623499362111},
  year = {2001},
  month = jan,
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  volume = {12},
  number = {1},
  pages = {109--138},
  author = "Angelia Nedi{\'{c}} and Dimitri P. Bertsekas",
  title = {{Incremental Subgradient Methods for Nondifferentiable Optimization}},
  journal = {{SIAM} Journal on Optimization}
}

@article{Nemirovski2009,
  author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
  title = {{Robust Stochastic Approximation Approach to Stochastic Programming}},
  journal = {SIAM Journal on Optimization},
  volume = {19},
  number = {4},
  pages = {1574-1609},
  year = {2009},
  doi = {10.1137/070704277}
}

@article{Bengio2012,
  title={{Practical Recommendations for Gradient-Based Training of Deep Architectures}},
  ISBN={9783642352898},
  ISSN={1611-3349},
  DOI={10.1007/978-3-642-35289-8_26},
  journal={Neural Networks: Tricks of the Trade},
  publisher={Springer Berlin Heidelberg},
  author={Bengio, Yoshua},
  year={2012},
  pages={437–478}
}

@InProceedings{nguyen2018sgd, 
  title = {{SGD and Hogwild! Convergence Without the Bounded Gradients Assumption}}, 
  author = {Nguyen, Lam and Nguyen, Phuong Ha and van Dijk, Marten and Richt{\'a}rik, Peter and Scheinberg, Katya and Tak\'{a}\v{c}, Martin}, 
  pages = {3750--3758}, 
  year = {2018}, 
  editor = {Jennifer Dy and Andreas Krause}, 
  volume = {80}, 
  series = {Proceedings of Machine Learning Research}, 
  address = {Stockholmsm\"{a}ssan, Stockholm Sweden}, 
  month = {10--15 Jul}, 
  publisher = {PMLR}
}

@article{Drori2019,
  title={{The Complexity of Finding Stationary Points with Stochastic Gradient Descent}},
  author={Yoel Drori and Ohad Shamir},
  year={2019},
  journal={arXiv preprint arXiv:1910.01845}
}

@incollection{HaNguyen2019,
  title = {{Tight Dimension Independent Lower Bound on the Expected Convergence Rate for Diminishing Step Sizes in SGD}},
  author = {Nguyen, Phuong Ha and Nguyen, Lam and van Dijk, Marten},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d' Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {3660--3669},
  year = {2019},
  publisher = {Curran Associates, Inc.},
}

@book{nemirovsky1983problem,
  title={{Problem Complexity and Method Efficiency in Optimization}},
  author={Nemirovsky, Arkadi and Yudin, David B.},
  year={1983},
  publisher={Wiley, New York}
}

@InProceedings{Rakhlin2012,
  author    = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  title     = {{Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization}},
  booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
  year      = {2012},
  series    = {ICML’12},
  pages     = {1571–1578},
  address   = {Madison, WI, USA},
  publisher = {Omnipress},
  isbn      = {9781450312851},
  location  = {Edinburgh, Scotland},
  numpages  = {8},
}

@article{Ahn2020,
    title={{On Tight Convergence Rates of Without-replacement SGD}},
    author={Kwangjun Ahn and Suvrit Sra},
    year={2020},
    journal={arXiv preprint arXiv:2004.08657},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@inproceedings{needell2014stochastic,
  title = {{Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm}},
  author = {Needell, Deanna and Ward, Rachel and Srebro, Nati},
  booktitle = {Advances in Neural Information Processing Systems 27},
  editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages = {1017--1025},
  year = {2014},
  publisher = {Curran Associates, Inc.},
}

@inproceedings{bottou2004large,
  title = {{Large Scale Online Learning}},
  author = {Bottou, L\'{e}on and Yann L. Cun},
  booktitle = {Advances in Neural Information Processing Systems 16},
  editor = {S. Thrun and L. K. Saul and B. Sch\"{o}lkopf},
  pages = {217--224},
  year = {2004},
  publisher = {MIT Press},
}

@article{malitsky2019adaptive,
  title={Adaptive gradient descent without descent},
  author={Malitsky, Yura and Mishchenko, Konstantin},
  journal={arXiv preprint arXiv:1910.09529},
  year={2019}
}

@article{Chen1993,
  author = {Chen, Gong and Teboulle, Marc},
  title = {{Convergence Analysis of a Proximal-Like Minimization Algorithm Using Bregman Functions}},
  journal = {SIAM Journal on Optimization},
  volume = {3},
  number = {3},
  pages = {538-543},
  year = {1993},
  doi = {10.1137/0803026},
}

@article{AhnYun2020,
    title={{SGD with shuffling: optimal rates without component convexity and large epoch requirements}}, 
    author={Kwangjun Ahn and Chulhee Yun and Suvrit Sra},
    year={2020},
    journal={arXiv preprint arXiv:2006.06946. To appear in NeurIPS 2020}
}

@inproceedings{loizou2020stochastic,
  title={Stochastic hamiltonian gradient methods for smooth games},
  author={Loizou, Nicolas and Berard, Hugo and Jolicoeur-Martineau, Alexia and Vincent, Pascal and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  booktitle={ICML},
  year={2020}
}

@article{chen1997convergence,
  title={Convergence rates in forward--backward splitting},
  author={Chen, George HG and Rockafellar, R Tyrrell},
  journal={SIAM Journal on Optimization},
  volume={7},
  number={2},
  pages={421--444},
  year={1997},
  publisher={SIAM}
}

@inproceedings{yu2022fast,
  title={Fast Distributionally Robust Learning with Variance-Reduced Min-Max Optimization},
  author={Yu, Yaodong and Lin, Tianyi and Mazumdar, Eric V and Jordan, Michael},
  booktitle={AISTATS},
  year={2022}
}

@inproceedings{mishchenko2020revisiting,
  title={Revisiting stochastic extragradient},
  author={Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richt{\'a}rik, Peter and Malitsky, Yura},
  booktitle={AISTATS},
  year={2020}
}

@inproceedings{namkoong2016stochastic,
  title={Stochastic gradient methods for distributionally robust optimization with f-divergences},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={NeurIPS},
  year={2016}
}

@article{brown2020combining,
  title={Combining deep reinforcement learning and search for imperfect-information games},
  author={Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{DBLP:conf/iclr/SokotaDKLLMBK23,
  author       = {Samuel Sokota and
                  Ryan D'Orazio and
                  J. Zico Kolter and
                  Nicolas Loizou and
                  Marc Lanctot and
                  Ioannis Mitliagkas and
                  Noam Brown and
                  Christian Kroer},
  title        = {A Unified Approach to Reinforcement Learning, Quantal Response Equilibria,
                  and Two-Player Zero-Sum Games},
  booktitle    = {ICLR 2023},
  year         = {2023}
}

@inproceedings{wang2021adversarial,
title={Adversarial Attack Generation Empowered by Min-Max Optimization},
author={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},
booktitle={NeurIPS},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021}
}

@inproceedings{goodfellow2014generativeadversarialnetworks,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      booktitle={NeurIPS},
      year={2014}
}

@inproceedings{
madry2018towards,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
booktitle={ICLR},
year={2018}
}

@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author =       {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {ICML},
  year = 	 {2017}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@article{han2023riemannian,
  title={Riemannian Hamiltonian methods for min-max optimization on manifolds},
  author={Han, Andi and Mishra, Bamdev and Jawanpuria, Pratik and Kumar, Pawan and Gao, Junbin},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={3},
  pages={1797--1827},
  year={2023},
  publisher={SIAM}
}

@inproceedings{balduzzi2018mechanics,
  title={The mechanics of n-player differentiable games},
  author={Balduzzi, David and Racaniere, Sebastien and Martens, James and Foerster, Jakob and Tuyls, Karl and Graepel, Thore},
  booktitle={ICML},
  year={2018}
}

@inproceedings{mescheder2017numerics,
  title={The numerics of gans},
  author={Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{gorbunov2022last,
  title={Last-iterate convergence of optimistic gradient method for monotone variational inequalities},
  author={Gorbunov, Eduard and Taylor, Adrien and Gidel, Gauthier},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{gorbunov2022stochastic,
  title={Stochastic extragradient: General analysis and improved rates},
  author={Gorbunov, Eduard and Berard, Hugo and Gidel, Gauthier and Loizou, Nicolas},
  booktitle={AISTATS},
  year={2022}
}

@inproceedings{beznosikov2022stochastic,
  title={Stochastic gradient descent-ascent: Unified theory and new efficient methods},
  author={Beznosikov, Aleksandr and Gorbunov, Eduard and Berard, Hugo and Loizou, Nicolas},
  booktitle={AISTATS},
  year={2023}
}

@inproceedings{daskalakis2018limit,
  title={The limit points of (optimistic) gradient descent in min-max optimization},
  author={Daskalakis, Constantinos and Panageas, Ioannis},
  booktitle={NeurIPS},
  year={2018}
}

@article{popov1980modification,
  title={A modification of the Arrow-Hurwicz method for search of saddle points},
  author={Popov, Leonid Denisovich},
  journal={Mathematical notes of the Academy of Sciences of the USSR},
  volume={28},
  number={5},
  pages={845--848},
  year={1980},
  publisher={Springer}
}

@inproceedings{gorbunov2022extragradient,
  title={Extragradient method: O (1/k) last-iterate convergence for monotone variational inequalities and connections with cocoercivity},
  author={Gorbunov, Eduard and Loizou, Nicolas and Gidel, Gauthier},
  booktitle={AISTATS},
    year={2022}
}

@inproceedings{pethick2023solving,
  title={Solving stochastic weak Minty variational inequalities without increasing batch size},
  author={Pethick, Thomas Michaelsen and Fercoq, Olivier and Latafat, Puya and Patrinos, Panagiotis and Cevher, Volkan},
  booktitle={ICLR},
  year={2023}
}

@article{cai2023empirical,
  title={Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds},
  author={Cai, Xufeng and Lin, Cheuk Yin and Diakonikolas, Jelena},
  journal={arXiv:2306.12498},
  year={2023}
}

@article{nguyen2021unified,
  title={A unified convergence analysis for shuffling-type gradient methods},
  author={Nguyen, Lam M and Tran-Dinh, Quoc and Phan, Dzung T and Nguyen, Phuong Ha and Van Dijk, Marten},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={9397--9440},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{beznosikov2022distributed,
  title={Distributed methods with compressed communication for solving variational inequalities, with theoretical guarantees},
  author={Beznosikov, Aleksandr and Richt{\'a}rik, Peter and Diskin, Michael and Ryabinin, Max and Gasnikov, Alexander},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{choudhury2023single,
 title={Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions},
 author={Sayantan Choudhury and Eduard Gorbunov and Nicolas Loizou},
 booktitle={NeurIPS},
 year={2023}
}

@inproceedings{zhang2023communication,
  title={Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates},
  author={Zhang, Siqi and Choudhury, Sayantan and Stich, Sebastian U and Loizou, Nicolas},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{hsieh2019convergence,
  title={On the convergence of single-call stochastic extra-gradient methods},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{li2022convergence,
  title={On the convergence of stochastic extragradient for bilinear games using restarted iteration averaging},
  author={Li, Chris Junchi and Yu, Yaodong and Loizou, Nicolas and Gidel, Gauthier and Ma, Yi and Le Roux, Nicolas and Jordan, Michael},
  booktitle={AISTATS},
    year={2022}
}

@inproceedings{mishchenko2020random,
  title={Random reshuffling: Simple analysis with vast improvements},
  author={Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
  booktitle={NeurIPS},
  year={2020}
}

@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@inproceedings{ahn2020sgd,
  title={Sgd with shuffling: optimal rates without component convexity and large epoch requirements},
  author={Ahn, Kwangjun and Yun, Chulhee and Sra, Suvrit},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{lin2020finite,
  title={Finite-time last-iterate convergence for multi-agent learning in games},
  author={Lin, Tianyi and Zhou, Zhengyuan and Mertikopoulos, Panayotis and Jordan, Michael},
  booktitle={ICML},
  year={2020}
}

@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={ICML},
    year={2020}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={AISTATS},
  year={2017}
}

@article{necoara2019linear,
  title={Linear convergence of first order methods for non-strongly convex optimization},
  author={Necoara, Ion and Nesterov, Yu and Glineur, Francois},
  journal={Mathematical Programming},
  volume={175},
  pages={69--107},
  year={2019},
  publisher={Springer}
}

@inproceedings{hsieh2020explore,
  title={Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{beznosikov2023stochastic,
  title={Stochastic gradient descent-ascent: Unified theory and new efficient methods},
  author={Beznosikov, Aleksandr and Gorbunov, Eduard and Berard, Hugo and Loizou, Nicolas},
  booktitle={AISTATS},
  year={2023}
}

@inproceedings{
daskalakis2018training,
title={Training {GAN}s with Optimism},
author={Constantinos Daskalakis and Andrew Ilyas and Vasilis Syrgkanis and Haoyang Zeng},
booktitle={ICLR},
year={2018}
}

@inproceedings{
chavdarova2021taming,
title={Taming {GAN}s with Lookahead-Minmax},
author={Tatjana Chavdarova and Matteo Pagliardini and Sebastian U Stich and Fran{\c{c}}ois Fleuret and Martin Jaggi},
booktitle={ICLR},
year={2021}
}

@article{Neumann1928,
author = {Neumann, J. von},
journal = {Mathematische Annalen},
pages = {295-320},
title = {Zur Theorie der Gesellschaftsspiele},
volume = {100},
year = {1928},
}

@article{Blackwell1956AnAO,
  title={An analog of the minimax theorem for vector payoffs.},
  author={David Blackwell},
  journal={Pacific Journal of Mathematics},
  year={1956},
  volume={6},
  pages={1-8}
}

@misc{cai2022tight,
      title={Tight Last-Iterate Convergence of the Extragradient and the Optimistic Gradient Descent-Ascent Algorithm for Constrained Monotone Variational Inequalities}, 
      author={Yang Cai and Argyris Oikonomou and Weiqiang Zheng},
      year={2022},
      archivePrefix={arXiv },
      eprint={2204.09228}
}

@inproceedings{das2022sampling,
  title={Sampling without replacement leads to faster rates in finite-sum minimax optimization},
  author={Das, Aniket and Sch{\"o}lkopf, Bernhard and Muehlebach, Michael},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{Jain2019SGDWR,
  title={SGD without Replacement: Sharper Rates for General Smooth Convex Functions},
  author={Prateek Jain and Dheeraj M. Nagaraj and Praneeth Netrapalli},
  booktitle={ICML},
  year={2019}
}

@inproceedings{Yang2020GlobalCA,
  title={Global Convergence and Variance Reduction for a Class of Nonconvex-Nonconcave Minimax Problems},
  author={Junchi Yang and Negar Kiyavash and Niao He},
  booktitle={NeurIPS},
  year={2020}
}

@article{gurbuzbalaban2021random,
  title={Why random reshuffling beats stochastic gradient descent},
  author={G{\"u}rb{\"u}zbalaban, Mert and Ozdaglar, Asu and Parrilo, Pablo A},
  journal={Mathematical Programming},
  volume={186},
  pages={49--84},
  year={2021},
  publisher={Springer}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  publisher={Springer}
}

@inproceedings{haochen2019random,
  title={Random shuffling beats sgd after finite epochs},
  author={Haochen, Jeff and Sra, Suvrit},
  booktitle={ICML},
  year={2019}
}

@article{RRSGDcontrol,
  title={Stochastic learning under random reshuffling with constant step-sizes},
  author={Ying Bicheng and Yuan, Kun and Vlaski, Stefan and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={2},
  pages={474--489},
  year={2018},
  publisher={IEEE}
}

@inproceedings{safran2020good,
  title={How good is SGD with random shuffling?},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={COLT},
  year={2020}
}

@InProceedings{gower2019sgd,
  title={SGD: General Analysis and Improved Rates}, 
  author={Robert Mansel Gower and Nicolas Loizou and Xun Qian and Alibek Sailanbayev and Egor Shulgin and Peter Richtarik},
  booktitle={AISTATS}, 
  year={2019}
}

@InProceedings{gower2021sgd,
  title = {SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation},
  author = {Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  booktitle = {AISTATS},
  year = {2021}
}

@article{khaled2023unified,
  title={Unified analysis of stochastic gradient methods for composite convex and smooth optimization},
  author={Khaled, Ahmed and Sebbouh, Othmane and Loizou, Nicolas and Gower, Robert M and Richt{\'a}rik, Peter},
  journal={Journal of Optimization Theory and Applications},
  volume={199},
  number={2},
  pages={499--540},
  year={2023},
  publisher={Springer}
}

@inproceedings{Bottou2012StochasticGD,
  title={Stochastic Gradient Descent Tricks},
  author={L{\'e}on Bottou},
  booktitle={Neural Networks},
  year={2012}
}

@InProceedings{pmlr-v132-abernethy21a,
  title = {Last-Iterate Convergence Rates for Min-Max Optimization: Convergence of Hamiltonian Gradient Descent and Consensus Optimization},
  author =       {Abernethy, Jacob and Lai, Kevin A. and Wibisono, Andre},
  booktitle = 	 {ALT},
  year = 	 {2021}
}

@inproceedings{Bottou2009CuriouslyFC,
  title={Curiously Fast Convergence of some Stochastic Gradient Descent Algorithms},
  author={L{\'e}on Bottou},
  year={2009}
}

@book{bertsekas2003convex,
  title={Convex analysis and optimization},
  author={Bertsekas, Dimitri and Nedic, Angelia and Ozdaglar, Asuman},
  volume={1},
  year={2003},
  publisher={Athena Scientific}
}

@article{chung1954stochastic,
  title={On a stochastic approximation method},
  author={Chung, Kai Lai},
  journal={The Annals of Mathematical Statistics},
  pages={463--483},
  year={1954},
  publisher={JSTOR}
}

@article{sacks1958asymptotic,
  title={Asymptotic distribution of stochastic approximation procedures},
  author={Sacks, Jerome},
  journal={The Annals of Mathematical Statistics},
  volume={29},
  number={2},
  pages={373--405},
  year={1958},
  publisher={JSTOR}
}

@article{fabian1968asymptotic,
  title={On asymptotic normality in stochastic approximation},
  author={Fabian, Vaclav},
  journal={The Annals of Mathematical Statistics},
  pages={1327--1332},
  year={1968},
  publisher={JSTOR}
}

@techreport{ruppert1988efficient,
  title={Efficient estimations from a slowly convergent Robbins-Monro process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}

@article{shapiro1989asymptotic,
  title={Asymptotic properties of statistical estimators in stochastic programming},
  author={Shapiro, Alexander},
  journal={The Annals of Statistics},
  pages={841--858},
  year={1989},
  publisher={JSTOR}
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@inproceedings{tripuraneni2018averaging,
  title={Averaging stochastic gradient descent on Riemannian manifolds},
  author={Tripuraneni, Nilesh and Flammarion, Nicolas and Bach, Francis and Jordan, Michael I},
  booktitle={Conference On Learning Theory},
  pages={650--687},
  year={2018},
  organization={PMLR}
}

@article{su2018statistical,
  title={Statistical inference for online learning and stochastic approximation via hierarchical incremental gradient descent},
  author={Su, Weijie and Zhu, Yuancheng},
  journal={arXiv preprint arXiv:1802.04876},
  pages={3},
  year={2018}
}

@article{toulis2017asymptotic,
  title={Asymptotic and finite-sample properties of estimators based on stochastic gradients},
  author={Toulis, Panos and Airoldi, Edoardo M},
  year={2017}
}

@article{fang2018online,
  title={Online bootstrap confidence intervals for the stochastic gradient descent estimator},
  author={Fang, Yixin and Xu, Jinfeng and Yang, Lei},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={78},
  pages={1--21},
  year={2018}
}

@article{schmidt2013fast,
  title={Fast convergence of stochastic gradient descent under a strong growth condition},
  author={Schmidt, Mark and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:1308.6370},
  year={2013}
}

@inproceedings{ma2018power,
  title={The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning},
  author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  booktitle={International Conference on Machine Learning},
  pages={3325--3334},
  year={2018},
  organization={PMLR}
}

@inproceedings{vaswani2019fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={1195--1204},
  year={2019},
  organization={PMLR}
}

@article{kifer1988random,
  title={Random perturbations of dynamical systems},
  author={Kifer, Yuri},
  journal={Nonlinear Problems in Future Particle Accelerators},
  volume={189},
  year={1988},
  publisher={World Scientific}
}

@article{benaim1996dynamical,
  title={A dynamical system approach to stochastic approximations},
  author={Benaim, Michel},
  journal={SIAM Journal on Control and Optimization},
  volume={34},
  number={2},
  pages={437--472},
  year={1996},
  publisher={SIAM}
}

@article{priouret1998remark,
  title={A remark on the stability of the LMS tracking algorithm},
  author={Priouret, P and Veretenikov, A Yu},
  journal={Stochastic analysis and applications},
  volume={16},
  number={1},
  pages={119--129},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{fort1999asymptotic,
  title={Asymptotic behavior of a Markovian stochastic algorithm with constant step},
  author={Fort, Jean-Claude and Pages, Gilles},
  journal={SIAM journal on control and optimization},
  volume={37},
  number={5},
  pages={1456--1482},
  year={1999},
  publisher={SIAM}
}

@article{aguech2000perturbation,
  title={On a perturbation approach for the analysis of stochastic tracking algorithms},
  author={Aguech, Rafik and Moulines, Eric and Priouret, Pierre},
  journal={SIAM Journal on Control and Optimization},
  volume={39},
  number={3},
  pages={872--899},
  year={2000},
  publisher={SIAM}
}

@article{tan2023online,
  title={Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval},
  author={Tan, Yan Shuo and Vershynin, Roman},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={58},
  pages={1--47},
  year={2023}
}

@article{dieuleveut2020bridging,
  title={Bridging the gap between constant step size stochastic gradient descent and Markov chains},
  author={Dieuleveut, Aymeric and Durmus, Alain and Bach, Francis},
  year={2020}
}

@inproceedings{chee2018convergence,
  title={Convergence diagnostics for stochastic gradient descent with constant learning rate},
  author={Chee, Jerry and Toulis, Panos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1476--1485},
  year={2018},
  organization={PMLR}
}

@article{dalalyan2019user,
  title={User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient},
  author={Dalalyan, Arnak S and Karagulyan, Avetik},
  journal={Stochastic Processes and their Applications},
  volume={129},
  number={12},
  pages={5278--5311},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{brosse2017sampling,
  title={Sampling from a log-concave distribution with compact support with proximal Langevin Monte Carlo},
  author={Brosse, Nicolas and Durmus, Alain and Moulines, {\'E}ric and Pereyra, Marcelo},
  booktitle={Conference on learning theory},
  pages={319--342},
  year={2017},
  organization={PMLR}
}

@article{cheng2018sharp,
  title={Sharp convergence rates for Langevin dynamics in the nonconvex setting},
  author={Cheng, Xiang and Chatterji, Niladri S and Abbasi-Yadkori, Yasin and Bartlett, Peter L and Jordan, Michael I},
  journal={arXiv preprint arXiv:1805.01648},
  year={2018}
}

@article{durmus2017nonasymptotic,
  title={Nonasymptotic convergence analysis for the unadjusted Langevin algorithm},
  author={Durmus, Alain and Moulines, Eric},
  year={2017}
}

@article{dalalyan2017theoretical,
  title={Theoretical guarantees for approximate sampling from smooth and log-concave densities},
  author={Dalalyan, Arnak S},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={79},
  number={3},
  pages={651--676},
  year={2017},
  publisher={Oxford University Press}
}

@inproceedings{cheng2018underdamped,
  title={Underdamped Langevin MCMC: A non-asymptotic analysis},
  author={Cheng, Xiang and Chatterji, Niladri S and Bartlett, Peter L and Jordan, Michael I},
  booktitle={Conference on learning theory},
  pages={300--323},
  year={2018},
  organization={PMLR}
}

@article{bubeck2018sampling,
  title={Sampling from a log-concave distribution with projected Langevin Monte Carlo},
  author={Bubeck, S{\'e}bastien and Eldan, Ronen and Lehec, Joseph},
  journal={Discrete \& Computational Geometry},
  volume={59},
  number={4},
  pages={757--783},
  year={2018},
  publisher={Springer}
}

@article{dwivedi2019log,
  title={Log-concave sampling: Metropolis-Hastings algorithms are fast},
  author={Dwivedi, Raaz and Chen, Yuansi and Wainwright, Martin J and Yu, Bin},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={183},
  pages={1--42},
  year={2019}
}

@article{dalalyan2020sampling,
  title={On sampling from a log-concave density using kinetic Langevin diffusions},
  author={Dalalyan, Arnak S and Riou-Durand, Lionel},
  year={2020}
}

@article{li2019stochastic,
  title={Stochastic runge-kutta accelerates langevin monte carlo and beyond},
  author={Li, Xuechen and Wu, Yi and Mackey, Lester and Erdogdu, Murat A},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{shen2019randomized,
  title={The randomized midpoint method for log-concave sampling},
  author={Shen, Ruoqi and Lee, Yin Tat},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{erdogdu2021convergence,
  title={On the convergence of langevin monte carlo: The interplay between tail growth and smoothness},
  author={Erdogdu, Murat A and Hosseinzadeh, Rasa},
  booktitle={Conference on Learning Theory},
  pages={1776--1822},
  year={2021},
  organization={PMLR}
}

@inproceedings{antonakopoulos2022adagrad,
  title={AdaGrad avoids saddle points},
  author={Antonakopoulos, Kimon and Mertikopoulos, Panayotis and Piliouras, Georgios and Wang, Xiao},
  booktitle={International Conference on Machine Learning},
  pages={731--771},
  year={2022},
  organization={PMLR}
}

@article{baudel2023hill,
  title={On the Hill relation and the mean reaction time for metastable processes},
  author={Baudel, Manon and Guyader, Arnaud and Leli{\`e}vre, Tony},
  journal={Stochastic Processes and their Applications},
  volume={155},
  pages={393--436},
  year={2023},
  publisher={Elsevier}
}

@incollection{benaim2006dynamics,
  title={Dynamics of stochastic approximation algorithms},
  author={Bena{\"\i}m, Michel},
  booktitle={Seminaire de probabilites XXXIII},
  pages={1--68},
  year={2006},
  publisher={Springer}
}

@article{benaim1995dynamics,
  title={Dynamics of Morse-Smale urn processes},
  author={Bena{\"\i}m, Michel and Hirsch, Morris W},
  journal={Ergodic Theory and Dynamical Systems},
  volume={15},
  number={6},
  pages={1005--1030},
  year={1995},
  publisher={Cambridge University Press}
}

@article{bertsekas2000gradient,
  title={Gradient convergence in gradient methods with errors},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  journal={SIAM Journal on Optimization},
  volume={10},
  number={3},
  pages={627--642},
  year={2000},
  publisher={SIAM}
}

@inproceedings{brandiere1996algorithmes,
  title={Les algorithmes stochastiques contournent-ils les pieges?},
  author={Brandi{\`e}re, Odile and Duflo, Marie},
  booktitle={Annales de l'IHP Probabilit{\'e}s et statistiques},
  volume={32},
  number={3},
  pages={395--427},
  year={1996}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on learning theory},
  pages={797--842},
  year={2015},
  organization={PMLR}
}

@article{gesu2019sharp,
  title={Sharp asymptotics of the first exit point density},
  author={Ges{\`u}, Giacomo Di and Leli{\`e}vre, Tony and Peutrec, Dorian Le and Nectoux, Boris},
  journal={Annals of PDE},
  volume={5},
  number={1},
  pages={5},
  year={2019},
  publisher={Springer}
}

@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in SGD},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  pages={3964--3975},
  year={2021},
  organization={PMLR}
}

@inproceedings{hodgkinson2021multiplicative,
  title={Multiplicative noise and heavy tails in stochastic optimization},
  author={Hodgkinson, Liam and Mahoney, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4262--4274},
  year={2021},
  organization={PMLR}
}

@inproceedings{hsieh2021limits,
  title={The limits of min-max optimization algorithms: Convergence to spurious non-critical sets},
  author={Hsieh, Ya-Ping and Mertikopoulos, Panayotis and Cevher, Volkan},
  booktitle={International Conference on Machine Learning},
  pages={4337--4348},
  year={2021},
  organization={PMLR}
}

@article{hsieh2023riemannian,
  title={Riemannian stochastic optimization methods avoid strict saddle points},
  author={Hsieh, Ya-Ping and Karimi Jaghargh, Mohammad Reza and Krause, Andreas and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={29580--29601},
  year={2023}
}

@article{hu2017diffusion,
  title={On the diffusion approximation of nonconvex stochastic gradient descent},
  author={Hu, Wenqing and Li, Chris Junchi and Li, Lei and Liu, Jian-Guo},
  journal={arXiv preprint arXiv:1705.07562},
  year={2017}
}

@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrzkebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{jordan1998variational,
  title={The variational formulation of the Fokker--Planck equation},
  author={Jordan, Richard and Kinderlehrer, David and Otto, Felix},
  journal={SIAM journal on mathematical analysis},
  volume={29},
  number={1},
  pages={1--17},
  year={1998},
  publisher={SIAM}
}

@article{kiefer1952stochastic,
  title={Stochastic estimation of the maximum of a regression function},
  author={Kiefer, Jack and Wolfowitz, Jacob},
  journal={The Annals of Mathematical Statistics},
  pages={462--466},
  year={1952},
  publisher={JSTOR}
}

@book{lan2020first,
  title={First-order and stochastic optimization methods for machine learning},
  author={Lan, Guanghui},
  volume={1},
  year={2020},
  publisher={Springer}
}

@article{li2020reconciling,
  title={Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate},
  author={Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14544--14555},
  year={2020}
}

@article{li2021happens,
  title={What Happens after SGD Reaches Zero Loss?--A Mathematical Framework},
  author={Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2110.06914},
  year={2021}
}

@article{li2022fast,
  title={Fast mixing of stochastic gradient descent with normalization and weight decay},
  author={Li, Zhiyuan and Wang, Tianhao and Yu, Dingli},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9233--9248},
  year={2022}
}

@article{liu2023aiming,
  title={Aiming towards the minimizers: fast convergence of SGD for overparametrized problems},
  author={Liu, Chaoyue and Drusvyatskiy, Dmitriy and Belkin, Misha and Davis, Damek and Ma, Yian},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={60748--60767},
  year={2023}
}

@article{ljung2003analysis,
  title={Analysis of recursive stochastic algorithms},
  author={Ljung, Lennart},
  journal={IEEE transactions on automatic control},
  volume={22},
  number={4},
  pages={551--575},
  year={2003},
  publisher={IEEE}
}

@article{ljung1978strong,
  title={Strong convergence of a stochastic approximation algorithm},
  author={Ljung, Lennart},
  journal={The Annals of Statistics},
  volume={6},
  number={3},
  pages={680--696},
  year={1978},
  publisher={Institute of Mathematical Statistics}
}

@article{marion2023implicit,
  title={Implicit regularization of deep residual networks towards neural ODEs},
  author={Marion, Pierre and Wu, Yu-Han and Sander, Michael E and Biau, G{\'e}rard},
  journal={arXiv preprint arXiv:2309.01213},
  year={2023}
}

@article{mertikopoulos2020almost,
  title={On the almost sure convergence of stochastic gradient descent in non-convex problems},
  author={Mertikopoulos, Panayotis and Hallak, Nadav and Kavis, Ali and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1117--1128},
  year={2020}
}

@article{mertikopoulos2024unified,
  title={A unified stochastic approximation framework for learning in games},
  author={Mertikopoulos, Panayotis and Hsieh, Ya-Ping and Cevher, Volkan},
  journal={Mathematical Programming},
  volume={203},
  number={1},
  pages={559--609},
  year={2024},
  publisher={Springer}
}

@article{mignacco2022effective,
  title={The effective noise of stochastic gradient descent},
  author={Mignacco, Francesca and Urbani, Pierfrancesco},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2022},
  number={8},
  pages={083405},
  year={2022},
  publisher={IOP Publishing}
}

@article{mignacco2020dynamical,
  title={Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification},
  author={Mignacco, Francesca and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9540--9550},
  year={2020}
}

@inproceedings{mori2022power,
  title={Power-law escape rate of SGD},
  author={Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
  booktitle={International Conference on Machine Learning},
  pages={15959--15975},
  year={2022},
  organization={PMLR}
}

@inproceedings{nguyen2017loss,
  title={The loss surface of deep and wide neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={International conference on machine learning},
  pages={2603--2612},
  year={2017},
  organization={PMLR}
}

@article{nguyen2018loss,
  title={On the loss landscape of a class of deep neural networks with no bad local valleys},
  author={Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  journal={arXiv preprint arXiv:1809.10749},
  year={2018}
}

@article{nguyen2020global,
  title={Global convergence of deep networks with one wide layer followed by pyramidal topology},
  author={Nguyen, Quynh N and Mondelli, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11961--11972},
  year={2020}
}

@article{nguyen2019first,
  title={First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise},
  author={Nguyen, Thanh Huy and Simsekli, Umut and Gurbuzbalaban, Mert and Richard, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{pavasovic2023approximate,
  title={Approximate heavy tails in offline (multi-pass) stochastic gradient descent},
  author={Pavasovic, Krunoslav Lehman and Durmus, Alain and Simsekli, Umut},
  journal={arXiv preprint arXiv:2310.18455},
  year={2023}
}

@article{pemantle1990nonconvergence,
  title={Nonconvergence to unstable points in urn models and stochastic approximations},
  author={Pemantle, Robin},
  journal={The Annals of Probability},
  pages={698--712},
  year={1990},
  publisher={JSTOR}
}

@inproceedings{staib2019escaping,
  title={Escaping saddle points with adaptive gradient methods},
  author={Staib, Matthew and Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={5956--5965},
  year={2019},
  organization={PMLR}
}

@article{veiga2024stochastic,
  title={Stochastic gradient flow dynamics of test risk and its exact solution for weak features},
  author={Veiga, Rodrigo and Remizova, Anastasia and Macris, Nicolas},
  journal={arXiv preprint arXiv:2402.07626},
  year={2024}
}

@article{vlaski2021second,
  title={Second-order guarantees of stochastic gradient descent in nonconvex optimization},
  author={Vlaski, Stefan and Sayed, Ali H},
  journal={IEEE Transactions on Automatic Control},
  volume={67},
  number={12},
  pages={6489--6504},
  year={2021},
  publisher={IEEE}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@article{pflug1986stochastic,
  title={Stochastic minimization with constant step-size: asymptotic laws},
  author={Pflug, Georg Ch},
  journal={SIAM Journal on Control and Optimization},
  volume={24},
  number={4},
  pages={655--666},
  year={1986},
  publisher={SIAM}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{yu2021analysis,
  title={An analysis of constant step size SGD in the non-convex regime: Asymptotic normality and bias},
  author={Yu, Lu and Balasubramanian, Krishnakumar and Volgushev, Stanislav and Erdogdu, Murat A},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4234--4248},
  year={2021}
}

@inproceedings{
cho2023sgda,
title={{SGDA} with shuffling: faster convergence for nonconvex-P{\L} minimax optimization},
author={Hanseul Cho and Chulhee Yun},
booktitle={ICLR},
year={2023}
}

@inproceedings{azizian2020tight,
  title={A tight and unified analysis of gradient-based methods for a whole spectrum of differentiable games},
  author={Azizian, Wa{\"\i}ss and Mitliagkas, Ioannis and Lacoste-Julien, Simon and Gidel, Gauthier},
  booktitle={International conference on artificial intelligence and statistics},
  pages={2863--2873},
  year={2020},
  organization={PMLR}
}

@article{bauschke2017convex,
  title={Convex Analysis and Monotone Operator Theory in Hilbert Spaces, 2011 Springer},
  author={Bauschke, HH and Combettes, PL},
  journal={New York},
  year={2017}
}

@book{facchinei2003finite,
  title={Finite-dimensional variational inequalities and complementarity problems},
  author={Facchinei, Francisco and Pang, Jong-Shi},
  year={2003},
  publisher={Springer}
}

@article{dantzig1968complementary,
  title={Complementary pivot theory of. mathematical programming},
  author={Dantzig, George B and Cottle, RW},
  journal={Mathematics of the decision sciences, part},
  volume={1},
  pages={115--136},
  year={1968}
}

@article{syrgkanis2015fast,
  title={Fast convergence of regularized learning in games},
  author={Syrgkanis, Vasilis and Agarwal, Alekh and Luo, Haipeng and Schapire, Robert E},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{stampacchia1964formes,
  title={Formes bilineaires coercitives sur les ensembles convexes},
  author={Stampacchia, Guido},
  journal={Comptes Rendus Hebdomadaires Des Seances De L Academie Des Sciences},
  volume={258},
  number={18},
  pages={4413},
  year={1964},
  publisher={GAUTHIER-VILLARS/EDITIONS ELSEVIER 23 RUE LINOIS, 75015 PARIS, FRANCE}
}

@inproceedings{
pethick2022escaping,
title={Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems},
author={Thomas Pethick and Puya Latafat and Panos Patrinos and Olivier Fercoq and Volkan Cevher},
booktitle={ICLR},
year={2022}
}

@article{durmus2016stochastic,
  title={Stochastic gradient richardson-romberg markov chain monte carlo},
  author={Durmus, Alain and Simsekli, Umut and Moulines, Eric and Badeau, Roland and Richard, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{sokota2022unified,
  title={A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games},
  author={Sokota, Samuel and D'Orazio, Ryan and Kolter, J Zico and Loizou, Nicolas and Lanctot, Marc and Mitliagkas, Ioannis and Brown, Noam and Kroer, Christian},
  booktitle={ICLR},
  year={2023}
}

@article{mangold2024refined,
  title={Refined Analysis of Federated Averaging's Bias and Federated Richardson-Romberg Extrapolation},
  author={Mangold, Paul and Durmus, Alain and Dieuleveut, Aymeric and Samsonov, Sergey and Moulines, Eric},
  journal={arXiv preprint arXiv:2412.01389},
  year={2024}
}

@book{hildebrand1987introduction,
  title={Introduction to numerical analysis},
  author={Hildebrand, Francis Begnaud},
  year={1987},
  publisher={Courier Corporation}
}

@article{sheshukova2024nonasymptotic,
  title={Nonasymptotic analysis of stochastic gradient descent with the richardson-romberg extrapolation},
  author={Sheshukova, Marina and Belomestny, Denis and Durmus, Alain and Moulines, Eric and Naumov, Alexey and Samsonov, Sergey},
  journal={arXiv preprint arXiv:2410.05106},
  year={2024}
}

@article{talay1990expansion,
  title={Expansion of the global error for numerical schemes solving stochastic differential equations},
  author={Talay, Denis and Tubaro, Luciano},
  journal={Stochastic analysis and applications},
  volume={8},
  number={4},
  pages={483--509},
  year={1990},
  publisher={Taylor \& Francis}
}

@inproceedings{goodfellow2014generative,
title={Generative adversarial nets},
author={Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
booktitle={NeurIPS},
year={2014}
}

@incollection{bottou2012stochastic,
  title={Stochastic gradient descent tricks},
  author={Bottou, L{\'e}on},
  booktitle={Neural networks: tricks of the trade: second edition},
  pages={421--436},
  year={2012},
  publisher={Springer}
}

@article{bally1996law,
  title={The law of the Euler scheme for stochastic differential equations: I. Convergence rate of the distribution function},
  author={Bally, Vlad and Talay, Denis},
  journal={Probability theory and related fields},
  volume={104},
  number={1},
  pages={43--60},
  year={1996},
  publisher={Springer}
}

@article{allmeier2024computing,
  title={Computing the bias of constant-step stochastic approximation with markovian noise},
  author={Allmeier, Sebastian and Gast, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={137873--137902},
  year={2024}
}

@article{zhang2024constant,
  title={Constant stepsize q-learning: Distributional convergence, bias and extrapolation},
  author={Zhang, Yixuan and Xie, Qiaomin},
  journal={arXiv preprint arXiv:2401.13884},
  year={2024}
}

@article{romberg1955vereinfachte,
  title={Vereinfachte numerische integration},
  author={Romberg, Werner},
  journal={Norske Vid. Selsk. Forh.},
  volume={28},
  pages={30--36},
  year={1955},
  publisher={Trondheim}
}

@article{richardson1911ix,
  title={IX. The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam},
  author={Richardson, Lewis Fry},
  journal={Philosophical Transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character},
  volume={210},
  number={459-470},
  pages={307--357},
  year={1911},
  publisher={The Royal Society London}
}

@inproceedings{huo2023bias,
  title={Bias and extrapolation in Markovian linear stochastic approximation with constant stepsizes},
  author={Huo, Dongyan and Chen, Yudong and Xie, Qiaomin},
  booktitle={Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
  pages={81--82},
  year={2023}
}

@article{kwon2024two,
  title={Two-timescale linear stochastic approximation: Constant stepsizes go a long way},
  author={Kwon, Jeongyeol and Dotson, Luke and Chen, Yudong and Xie, Qiaomin},
  journal={arXiv preprint arXiv:2410.13067},
  year={2024}
}

@article{giannou2022convergence,
  title={On the convergence of policy gradient methods to Nash equilibria in general stochastic games},
  author={Giannou, Angeliki and Lotidis, Kyriakos and Mertikopoulos, Panayotis and Vlatakis-Gkaragkounis, Emmanouil-Vasileios},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7128--7141},
  year={2022}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={ICML},
  year={2017}
}

@misc{diakonikolas2021efficient,
      title={Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization}, 
      author={Jelena Diakonikolas and Constantinos Daskalakis and Michael I. Jordan},
      year={2021},
        booktitle={AISTATS},
}

@misc{huang2023monotonevariationalinequalitiessolution,
      title={Beyond Monotone Variational Inequalities: Solution Methods and Iteration Complexities}, 
      author={Kevin Huang and Shuzhong Zhang},
      year={2023}
}

@misc{karimi2020linearconvergencegradientproximalgradient,
      title={Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\L{}ojasiewicz Condition}, 
      author={Hamed Karimi and Julie Nutini and Mark Schmidt},
      year={2020}
}

@article{song2020optimistic,
  title={Optimistic dual extrapolation for coherent non-monotone variational inequalities},
  author={Song, Chaobing and Zhou, Zhengyuan and Zhou, Yichao and Jiang, Yong and Ma, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14303--14314},
  year={2020}
}

@article{erdogdu2018global,
  title={Global non-convex optimization with discretized diffusions},
  author={Erdogdu, Murat A and Mackey, Lester and Shamir, Ohad},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{rakhlin2011making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1109.5647},
  year={2011}
}

@inproceedings{raginsky2017non,
  title={Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1674--1703},
  year={2017},
  organization={PMLR}
}

@article{MalickMertikopoulos2024,
  title   = {The Global Convergence Time of Stochastic Gradient Descent in Non-Convex Landscapes: Sharp Estimates via Large Deviations},
  author  = {J{\'e}r{\^o}me Malick and Panayotis Mertikopoulos},
  journal = {arXiv preprint: 2503.16398}, 
  year    = {2024}
}

@article{azizian2024long,
  title={What is the long-run distribution of stochastic gradient descent? A large deviations analysis},
  author={Azizian, Wa{\"\i}ss and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={arXiv preprint arXiv:2406.09241},
  year={2024}
}

@article{mertikopoulos2019learning,
  title={Learning in games with continuous action sets and unknown payoff functions},
  author={Mertikopoulos, Panayotis and Zhou, Zhengyuan},
  journal={Mathematical Programming},
  volume={173},
  number={1},
  pages={465--507},
  year={2019},
  publisher={Springer}
}

@article{
bohm2023solving,
title={Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions},
author={Axel B{\"o}hm},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023}
}

@inproceedings{loizou2021stochastic,
  title={Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity},
  author={Loizou, Nicolas and Berard, Hugo and Gidel, Gauthier and Mitliagkas, Ioannis and Lacoste-Julien, Simon},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{daskalakis_complexity,
title = {The complexity of constrained min-max optimization},
author = {Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
booktitle = {STOC},
year = {2021}
}

@inproceedings{vlatakis2024stochastic,
  title={Stochastic methods in variational inequalities: Ergodicity, bias and refinements},
  author={Vlatakis-Gkaragkounis, Emmanouil Vasileios and Giannou, Angeliki and Chen, Yudong and Xie, Qiaomin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4123--4131},
  year={2024},
  organization={PMLR}
}

@inproceedings{emmanouilidis2024stochastic,
  title={Stochastic extragradient with random reshuffling: Improved convergence for variational inequalities},
  author={Emmanouilidis, Konstantinos and Vidal, Ren{\'e} and Loizou, Nicolas},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3682--3690},
  year={2024},
  organization={PMLR}
}

@article{hao2021gradient,
  title={A gradient descent method for solving a system of nonlinear equations},
  author={Hao, Wenrui},
  journal={Applied Mathematics Letters},
  volume={112},
  pages={106739},
  year={2021},
  publisher={Elsevier}
}

@article{pfau2016connecting,
  title={Connecting generative adversarial networks and actor-critic methods},
  author={Pfau, David and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1610.01945},
  year={2016}
}

@article{daskalakis2017training,
  title={Training gans with optimism},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  journal={arXiv preprint arXiv:1711.00141},
  year={2017}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@inproceedings{wiki,
    author= {Wikipedia}, 
    title = {Wasserstein metric – wikipedia, the free encyclopedia, 2023},
    publisher = {URL https://en.wikipedia.org/wiki/Wasserstein\_metric}, 
    year={Accessed: 2025-08-28}
}

@book{meyn2012markov,
  title={Markov chains and stochastic stability},
  author={Meyn, Sean P and Tweedie, Richard L},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

