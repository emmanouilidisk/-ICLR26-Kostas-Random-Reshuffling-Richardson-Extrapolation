% !TEX root = ./iclr2026_conference.tex
\section{Additional Related Literature}
\label{app:additional_related}
Owing to its central role in optimization and machine learning, stochastic gradient descent (SGD) and its numerous variants have generated an extensive body of literature that spans several decades.  
A complete survey is well beyond the scope of this paper, so we restrict ourselves to highlighting the most relevant threads and pointers.  

The only aspects we emphasize at this point are the phenomena most pertinent to our analysis: 
\begin{enumerate} 
\item classical stochastic approximation and asymptotic normality results;  
\item constant step-size schemes viewed through the lens of Markov chains and diffusion approximations;  
\item the widespread use of random reshuffling and its still-developing theoretical guarantees; and  
\item challenges that arise in min–max and variational inequality settings.  
\end{enumerate}
These themes form the backbone of our extended discussion in the appendix, where we provide a more comprehensive account of prior work.

\paragraph{From classical stochastic approximation to modern SGD.}
The study of stochastic approximation predates machine learning by decades, beginning with the foundational work of \citet{robbins1951stochastic} and \citet{kiefer1952stochastic}. Early analyses focused on vanishing step-sizes obeying the classic $L^2$--$L^1$ summability rules, and developed the ODE method to describe limiting dynamics; see, e.g., \citet{ljung1978strong,ljung2003analysis,benaim2006dynamics,bertsekas2000gradient}. In parallel, a rich line of results examined the almost-sure behavior of stochastic approximation, including avoidance of saddle points and convergence to locally stable equilibria \citep{pemantle1990nonconvergence,brandiere1996algorithmes,benaim1995dynamics,hsieh2021limits,hsieh2023riemannian,jordan1998variational,mertikopoulos2020almost,mertikopoulos2024unified,staib2019escaping,antonakopoulos2022adagrad}. 

\paragraph{Asymptotic normality and statistical inference.}
A complementary thread established central limit theorems for stochastic approximation: classical milestones include \citet{chung1954stochastic,sacks1958asymptotic,fabian1968asymptotic,ruppert1988efficient,shapiro1989asymptotic}, culminating in the Polyak--Juditsky averaging principle \citep{polyak1992acceleration}. Under suitable decaying step-sizes, the averaged SGD iterate is asymptotically normal and attains the Cramér--Rao optimal variance. This statistical perspective has been leveraged to construct confidence intervals and inference procedures for SGD-based estimators \citep{tripuraneni2018averaging,su2018statistical,toulis2017asymptotic,fang2018online}.

\paragraph{Constant step sizes: bias, speed, and Markovian viewpoints.}
Constant step-size policies, now standard in large-scale learning, trade a nonvanishing asymptotic error for fast initial progress and robust practical performance. Their benefits in over-parameterized regimes are well documented \citep{schmidt2013fast,needell2014stochastic,ma2018power,vaswani2019fast}. The Markov chain viewpoint provides a unifying language for analyzing such constant-step schemes: early developments used dynamical-systems and Markov-process techniques to establish stability and ergodic properties \citep{kifer1988random,benaim1996dynamical,priouret1998remark,fort1999asymptotic,aguech2000perturbation}, with recent refinements quantifying convergence behavior and variance \citep{dieuleveut2020bridging,chee2018convergence,tan2023online}. In parallel, diffusion-based analyses and Langevin-type discretizations connect SGD to MCMC methodology, yielding non-asymptotic guarantees and sharp mixing rates in log-concave and beyond-log-concave settings \citep{dalalyan2017theoretical,durmus2017nonasymptotic,cheng2018underdamped,dalalyan2019user,brosse2017sampling,cheng2018sharp,bubeck2018sampling,dwivedi2019log,dalalyan2020sampling,li2019stochastic,shen2019randomized,erdogdu2021convergence}.

\paragraph{Random reshuffling vs.\ with-replacement sampling.}
Among finite-sum methods, \emph{random reshuffling} (RR) occupies a special place: each epoch processes every component exactly once in a random order, in contrast to classical with-replacement SGD. RR is ubiquitous in practice---it improves cache locality \citep{Bengio2012}, often converges faster than with-replacement sampling \citep{Bottou2009,Recht2013}, and is the default in deep learning pipelines \citep{Sun2020}. The success of RR contrasts with the mature theory for with-replacement SGD, which enjoys tight upper/lower bounds in many regimes \citep{Rakhlin2012,Drori2019,HaNguyen2019}. A key analytical hurdle is bias: within an epoch, conditional expectations are no longer unbiased gradients, so classical SGD proofs do not transfer verbatim. Early attempts leveraging the noncommutative arithmetic--geometric mean conjecture \citep{RR-conjecture2012} were later undermined when the conjecture was disproved \citep{Recht-Re_conj_is_false_ICML_2020}. More recent works establish rates for twice-smooth and smooth objectives and highlight gaps between theory and prevalent heuristics \citep{Gurbuzbalaban2019RR,haochen2018random,Nagaraj2019,Safran2020good,Rajput2020}.

\paragraph{Incremental/ordered passes and sensitivity to permutations.}
Before RR became the default, incremental gradient (IG) methods with fixed orderings were widely used in neural network training \citep{Luo1991,Grippo1994}, with asymptotic convergence known since early work \citep{Mangasarian1994,Bertsekas2000}. However, their performance can depend strongly on the chosen ordering \citep{Nedic2001,Bertsekas2011}. By randomizing the order every epoch, RR mitigates this sensitivity and—under smoothness—can outperform both with-replacement SGD and deterministic IG \citep{Gurbuzbalaban2019RR,haochen2018random}, with refined lower/upper bounds developed in follow-up studies \citep{Nagaraj2019,Safran2020good,Rajput2020}.

\paragraph{Min–max problems and variational inequalities.}
In large-scale saddle-point and VIP settings, most theoretical analyses assume with-replacement sampling for convenience, whereas implementations overwhelmingly adopt without-replacement sampling \citep{Bottou2012StochasticGD}. A growing literature is closing this gap: for minimization problems, several works show (sometimes provably faster) RR rates in finite-sum regimes \citep{mishchenko2020random,ahn2020sgd,gurbuzbalaban2021random,cai2023empirical}. For min–max and VIPs, guarantees remain comparatively sparse: \citet{chen1997convergence} and \citet{korpelevich1976extragradient} initiated the study of stochastic and extragradient-type methods, with modern analyses for SEG and optimistic variants \citep{gorbunov2022extragradient,gorbunov2022last,hsieh2019convergence,choudhury2023single}. For RR specifically, \citet{das2022sampling} derive guarantees for SGDA and PPM under strong structural conditions, and \citet{cho2023sgda} extend to certain non-monotone settings. Nevertheless, classical SGDA can diverge even in simple monotone bilinear games, while proximal methods are implicit and less practical; filling this theoretical–practical gap remains an active direction.

\paragraph{Overparameterization and global convergence phenomena.}
Finally, SGD training dynamics in overparameterized neural networks reveal regimes where global convergence can emerge from structural properties such as width, depth, and initialization \citep{du2019gradient,zou2020gradient,nguyen2020global,liu2023aiming}. These results are powerful but specialized: they rely on problem-specific structure (e.g., width scaling or tailored initializations). Our focus is orthogonal—we seek guarantees for general non-convex or non-monotone landscapes under stochastic approximation, independent of architectural assumptions. For completeness, we refer the reviewer for the related work of the aforementioned work for further  surveys about these SGD \& overparameterization results in more detail.

\textbf{Comparison to Prior Work and Overview of Our Contributions.}
Before introducing the intuition behind our algorithmic design, we briefly contrast our results with those of \cite{emmanouilidis2024stochastic}, who study \RRresh-based improvements for the Stochastic Extragradient (SEG) method. 
Our analysis uncovers a fundamentally different phenomenon: the joint use of $\RRresh \oplus \RRrom$ produces a \emph{bias cancellation mechanism} that eliminates the leading $\mathcal{O}(\gamma)$ term while preserving the condition number and asymptotic behavior of SGDA. 
The key distinctions are summarized in Table~\ref{tab:comparison}. 
Achieving the best of both worlds—optimal bias order together with a tight condition number, as SEG without reshuffling enjoys—remains an interesting direction for future work.

% \vspace{-0.35cm}

\begin{table}[h!]
\centering
\scalebox{0.92}{
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{\cite{emmanouilidis2024stochastic}} & \textbf{Our work} \\
\midrule

Baseline Algorithm & \textbf{SEG} & \textbf{SGDA} \\[2pt]

Model Assumptions (Smoothness) &
$F_i$--$L_i$ Lipschitz &
$F_i$--$L_i$ Lipschitz
\\[2pt]

Model Assumptions (Drift) &
$F$ $\mu$--strongly monotone &
$F$ \emph{quasi}--strongly monotone
\\[2pt]

Main heuristic & \textbf{\RRresh\ only} & \textbf{\RRresh\ $\oplus$ \RRrom} \\[2pt]

Asymptotic Bias order & $\mathcal{O}(\gamma + \gamma^3)$ & $\mathcal{O}(\gamma^3)$ \\[2pt]

Asymptotic MSE order & $\mathcal{O}(\gamma^2)$ & $\mathcal{O}(\gamma^2)$ \\[2pt]

Condition number & \textbf{Worse than vanilla-SEG} & \textbf{Same as vanilla-SGDA} \\[2pt]

Mechanism & EG-structure + $\RRresh$ & Bias cancellation ($\RRresh \oplus \RRrom$) \\
\bottomrule
\end{tabular}
} % end scalebox
\caption{Summary of key differences between Emmanouilidis--Vidal--Loizou (2024) and our results.}
\label{tab:comparison}
\end{table}
% \vspace{-2em}

\medskip
\noindent\textit{Summary.} 
To summarize, there is a mature theory for with-replacement SGD (both asymptotic and non-asymptotic), well-developed statistical limits via averaging, and powerful diffusion/Markov perspectives for constant-step schemes. RR, despite being the practical default, poses distinctive analytical challenges due to its within-epoch bias, especially in min–max and VIP settings. Recent advances begin to bridge this gap, but a comprehensive understanding of how classical heuristics (constant steps, reshuffling, extrapolation) interact remains incomplete—precisely the juncture where our work contributes.
%%%%% !TEX root = ./iclr2026_conference.tex
%%%%\section{Additional Related Literature}
%%%%
%%%%The study of (SGD) goes back to the seminal work of Robbins \& Monro \citep{robbins1951stochastic} and Kiefer \& Wolfowitz \citep{kiefer1952stochastic}, who introduced the method in the context of solving systems of nonlinear equations in the 1950’s. Originally, the analysis of (SGD) involved a vanishing step-size $\eta_n$ satisfying the ``$L^2 - L^1$'' summability conditions $\sum_n \eta_n^2 < \infty = \sum_n \eta_n$, and gave rise to the ODE method of stochastic approximation. In this context, the first convergence results for (SGD) were obtained by Ljung \citep{ljung2003analysis,ljung1978strong}, Benaïm \citep{benaim2006dynamics}, and Bertsekas \& Tsitsiklis \citep{bertsekas2000gradient}, who established the almost sure convergence of the method in non-convex problems (with different regularity conditions for $f$). In conjunction with the above, a parallel thread in the literature launched by Pemantle \citep{pemantle1990nonconvergence} and Brandi{\`e}re \& Duflo \citep{brandiere1996algorithmes} showed that (SGD) avoids saddle points with probability 1, so, barring degeneracies, it only converges to local minimizers of $f$ -- see also \citep{antonakopoulos2022adagrad,benaim1995dynamics,hsieh2021limits,hsieh2023riemannian,jordan1998variational,mertikopoulos2020almost,mertikopoulos2024unified,staib2019escaping} and references therein.
%%%%
%%%%On the other hand, when (SGD) is run with a constant step-size -- the standard choice in data science and machine learning -- the situation is drastically different. The trajectories of (SGD) do not converge, but they instead wander around the problem’s state space, spending most time near the critical points of $f$. This is quantified by ``criticality bounds'' of the form 
%%%%$
%%%%\mathbb{E}\!\left[\sum_{k=0}^n \|\nabla f(x_k)\|^2\right] = \mathcal{O}(\sqrt{n}),
%%%%$
%%%%which certify an output with small gradient norm, in expectation or with high probability \citep{lan2020first}. As in the vanishing step-size regime, these results are supplemented by a range of saddle-point avoidance results \citep{ge2015escaping,vlaski2021second} which, under certain conditions, imply that the output of (SGD) is approximately second-order optimal (and hence, in most cases, a near-minimizer).
%%%%
%%%%Owing to its importance, (SGD) and its variants have given rise to a vast corpus of literature which cannot be adequately surveyed here. As we mentioned above, in the non-convex case, most of this literature concerns the criticality and saddle-point avoidance guarantees of the method, under different structural and regularity assumptions. For our purposes, the most relevant threads in the literature revolve around 
%%%%(i) treating $x_n$ as a discrete-time Markov chain and examining its tails \citep{gurbuzbalaban2021heavy,hodgkinson2021multiplicative,pemantle1990nonconvergence}, 
%%%%(ii) viewing it as a discrete-time approximation of a stochastic differential equation (SDE) and employing tools like dynamic mean-field theory (DMFT) to study the resulting ``diffusion approximation'' limit \citep{mignacco2022effective,mignacco2020dynamical,veiga2024stochastic}; and/or 
%%%%(iii) focusing on the time it takes (SGD) to escape a spurious local minimum \citep{gesu2019sharp,du2019gradient,hu2017diffusion,jastrzkebski2017three,mori2022power,xie2020diffusion}. 
%%%%Our analysis shares the same high-level goal as these general threads -- that is, understanding the global convergence properties of (SGD) in non-convex landscapes -- but we are not aware of any comparable results. To streamline our presentation, we provide a more detailed account of this literature in Appendix~A.
%%%%
%%%%The only thing we should highlight at this point is a range of phenomena that arise in the context of neural network training, where overparameterization and Gaussian initialization schemes can lead to global convergence \citep{,du2019gradient,zou2020gradient}. Results of this kind typically require some specific structure on the underlying neural network: a width scaling quadratically with the data \citep{nguyen2020global} -- or linearly for infinite-depth networks-- and/or initialization schemes that are attuned to the network’s structure \citep{liu2023aiming,nguyen2020global}. By contrast, our work takes a more holistic viewpoint as we aim to obtain results for general non-convex landscapes, without making any structural assumptions about the problem’s objective or the algorithm’s initialization. To provide the necessary context, we survey the relevant literature on overparameterized neural networks in Appendix~A.
%%%%
%%%%\paragraph{(SGD) as a discrete-time Markov chain.}
%%%%In a relatively recent thread in the literature, Dieuleveut et al. \citep{dieuleveut2020bridging} and Lu et al. \citep{yu2021analysis} undertook a study of (SGD) as a discrete-time Markov chain. This allowed Dieuleveut et al. \citep{dieuleveut2020bridging}, Lu et al. \citep{yu2021analysis} to derive conditions under which (SGD) is (geometrically) ergodic and, in this way, to quantify the distance to the minimizer under global growth conditions. Building further on this perspective, Gurbuzbalaban et al. \citep{gurbuzbalaban2021heavy}, Hodgkinson \& Mahoney \citep{hodgkinson2021multiplicative} and Pavasovic et al. \citep{pavasovic2023approximate} showed that, under general conditions, the asymptotic distribution of the iterates of (SGD) is heavy-tailed. As such, these results concern the probability of observing the iterates of (SGD) at very large distances from the origin. This is in contrast with our work, which focuses on the time it takes (SGD) to reach a global minimizer. These two types of results are orthogonal and complementary to each other.
%%%%
%%%%
%%%%\paragraph{Asymptotic Normality and Constant Step Sizes.}  
%%%%The study of asymptotic distributions for stochastic approximation can be traced back to the classical works of~\citet{chung1954stochastic,sacks1958asymptotic,fabian1968asymptotic,ruppert1988efficient,shapiro1989asymptotic}, with the seminal contribution of~\citet{polyak1992acceleration} providing a definitive characterization for strongly convex objectives.  
%%%%In particular,~\citet{ruppert1988efficient,polyak1992acceleration} showed that averaged SGD iterates, under a suitably decaying step size, converge in distribution to a Gaussian limit whose variance achieves the Cramér–Rao lower bound.  
%%%%This insight has since been leveraged for statistical inference with SGD, including the construction of confidence intervals~\citep{tripuraneni2018averaging,su2018statistical,toulis2017asymptotic,fang2018online}.  
%%%%
%%%%Alongside this line, another thread of research has emphasized the role of a \emph{constant step size}.  
%%%%Although such schemes do not remove the bias entirely, they offer faster convergence in over-parametrized regimes, as demonstrated in~\citet{schmidt2013fast,needell2014stochastic,ma2018power,vaswani2019fast}.  
%%%%A complementary perspective uses Markov chain tools to analyze constant-step stochastic approximation.  
%%%%This approach has roots in early works such as~\citet{kifer1988random,benaim1996dynamical,priouret1998remark,fort1999asymptotic,aguech2000perturbation}, and continues through more recent refinements like~\citet{tan2023online}.  
%%%%Further results on asymptotic variance for constant step-size SGD have been obtained by~\citet{dieuleveut2020bridging,chee2018convergence}.  
%%%%
%%%%Finally, there exists a vast related literature on diffusion-based perspectives, particularly in the context of Markov chain Monte Carlo algorithms.  
%%%%See, for instance,~\citet{dalalyan2019user,brosse2017sampling,cheng2018sharp,durmus2017nonasymptotic,dalalyan2017theoretical,cheng2018underdamped,bubeck2018sampling,dwivedi2019log,dalalyan2020sampling,li2019stochastic,shen2019randomized,erdogdu2021convergence} and the references therein, which highlight methodological connections between SGD analysis and Langevin-type sampling schemes.
%%%%
%%%%\paragraph{Random Reshuffling.}  
%%%%Among stochastic methods, \emph{random reshuffling} (RR) occupies a special position as the practical default in large-scale learning, yet its theoretical behavior remains comparatively less understood.  
%%%%In contrast to classical stochastic gradient descent (SGD), where each index is drawn independently with replacement at every step, RR enforces a full pass over all $n$ samples through a random permutation at each epoch.  
%%%%This simple change is known to bring tangible benefits: it often outperforms SGD in practice~\citep{Bottou2009,Recht2013}, improves memory locality~\citep{Bengio2012}, and has become the standard routine in modern deep learning pipelines~\citep{Sun2020}.  
%%%%
%%%%The success of RR is striking when compared to the well-established theory of SGD, for which tight upper and lower convergence bounds are available in many settings~\citep{Rakhlin2012,Drori2019,HaNguyen2019}.  
%%%%In RR, however, gradients within an epoch are no longer conditionally unbiased, since
%%%%$\ex[\nabla f_{\pi_i}(x_t^i)\mid x_t^i] \;\neq\; \nabla f(x_t^i), \quad i>0,$
%%%%so intermediate steps cannot be viewed as approximating true gradient descent.  
%%%%This bias has long complicated attempts at analysis: early approaches linked to the noncommutative arithmetic–geometric mean conjecture~\citep{RR-conjecture2012} eventually collapsed when the conjecture was disproved~\citep{Recht-Re_conj_is_false_ICML_2020}.  
%%%%More recent work has established rates in certain smooth regimes, though as \citet{Safran2020good} emphasize, even basic shuffling-only variants still lack direct convergence guarantees, with available results inherited indirectly from incremental gradient (IG) analyses.  
%%%%The empirical use of such heuristics, as noted by~\citet{Rajput2020}, remains ahead of what current theory can justify.
%%%%
%%%%Historically, IG methods~\citep{Luo1991,Grippo1994} were already prominent in training neural networks, with asymptotic convergence established early~\citep{Mangasarian1994,Bertsekas2000}.  
%%%%Their performance, however, depends heavily on the chosen order of samples~\citep{Nedic2001,Bertsekas2011}.  
%%%%By contrast, RR mitigates this sensitivity through randomization, and for twice-smooth objectives, it has been shown to outperform both SGD and IG~\citep{Gurbuzbalaban2019RR,haochen2018random}.  
%%%%Later refinements sharpened rates for smooth cases~\citep{Nagaraj2019,Safran2020good,Rajput2020}.  
%%%%
%%%%The contrast between practice and theory becomes even sharper in the context of min-max optimization and variational inequalities (VIPs).  
%%%%In most stochastic approximation algorithms—including gradient descent~\citep{chen1997convergence}, extragradient~\citep{korpelevich1976extragradient,gorbunov2022extragradient}, and optimistic methods~\citep{popov1980modification,gorbunov2022last}—with-replacement sampling is the default assumption, while practical implementations overwhelmingly adopt without-replacement sampling~\citep{Bottou2012StochasticGD}.  
%%%%A growing literature has begun to bridge this gap by proving faster rates for RR in finite-sum minimization~\citep{mishchenko2020random,ahn2020sgd,gurbuzbalaban2021random,cai2023empirical}.  
%%%%Yet for min-max problems, results are sparse: \citet{das2022sampling} establish guarantees for SGDA and proximal methods under strong assumptions, and \citet{cho2023sgda} extend analysis to structured non-monotone problems.  
%%%%Still, fundamental challenges remain, as classical SGDA diverges even on simple monotone bilinear games, while proximal schemes are implicit and less practical.  
%%%%Thus, while RR dominates practice, its theoretical landscape in min-max optimization and VIPs is still incomplete.